import 'jsr:@supabase/functions-js/edge-runtime.d.ts';
import { createClient } from 'jsr:@supabase/supabase-js@2';

// AI dev note: Edge Function para transcri√ß√£o de √°udio usando OpenAI Whisper
// Busca prompt e configura√ß√µes da tabela ai_prompts no banco

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers':
    'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'POST, OPTIONS',
};

interface TranscribeRequest {
  audioBase64: string;
  audioType: string;
  language?: string;
}

interface TranscribeResponse {
  success: boolean;
  transcription?: string;
  error?: string;
  audioSize?: number;
  metadata?: {
    model: string;
    promptSource: string;
  };
}

Deno.serve(async (req: Request) => {
  // Handle CORS preflight
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  if (req.method !== 'POST') {
    return new Response(
      JSON.stringify({ success: false, error: 'M√©todo n√£o permitido' }),
      {
        status: 405,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      }
    );
  }

  try {
    // Parse request
    const body = await req.json();
    const { audioBase64, audioType = 'audio/webm' }: TranscribeRequest = body;

    if (!audioBase64) {
      return new Response(
        JSON.stringify({
          success: false,
          error: 'Audio base64 n√£o fornecido',
        }),
        {
          status: 400,
          headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        }
      );
    }

    // Convert base64 to Blob
    const base64Data = audioBase64.includes(',')
      ? audioBase64.split(',')[1]
      : audioBase64;
    const binaryString = atob(base64Data);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    const audioBlob = new Blob([bytes], { type: audioType });

    // AI dev note: Verificar tamanho do √°udio (limite do Whisper √© 25MB)
    const MAX_SIZE = 25 * 1024 * 1024; // 25MB
    if (audioBlob.size > MAX_SIZE) {
      console.error(
        `‚ùå Arquivo muito grande: ${(audioBlob.size / 1024 / 1024).toFixed(2)}MB`
      );
      return new Response(
        JSON.stringify({
          success: false,
          error: `Arquivo de √°udio muito grande (${(audioBlob.size / 1024 / 1024).toFixed(1)}MB). O limite √© 25MB. Tente gravar um √°udio mais curto ou com menor qualidade.`,
        }),
        {
          status: 400,
          headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        }
      );
    }

    console.log(
      `üìä Tamanho do √°udio: ${(audioBlob.size / 1024 / 1024).toFixed(2)}MB, Tipo: ${audioType}`
    );

    // Get environment variables
    const supabaseUrl = Deno.env.get('SUPABASE_URL');
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY');

    if (!supabaseUrl || !supabaseServiceKey) {
      throw new Error('Vari√°veis de ambiente do Supabase n√£o encontradas');
    }

    // Create Supabase client
    const supabase = createClient(supabaseUrl, supabaseServiceKey);

    // Fetch OpenAI key from Supabase
    const { data: apiKeys, error: apiError } = await supabase
      .from('api_keys')
      .select('encrypted_key')
      .eq('service_name', 'openai')
      .eq('is_active', true)
      .single();

    if (apiError || !apiKeys?.encrypted_key) {
      throw new Error('Chave OpenAI n√£o encontrada');
    }

    const openaiKey = apiKeys.encrypted_key;

    // Fetch transcription prompt and model from ai_prompts table
    const { data: promptData, error: promptError } = await supabase
      .from('ai_prompts')
      .select('prompt_content, openai_model')
      .eq('prompt_name', 'audio_transcription')
      .eq('is_active', true)
      .single();

    // AI dev note: Default para gpt-4o-transcribe (melhor qualidade dispon√≠vel)
    let promptContent =
      'A fisioterapeuta est√° gravando uma evolu√ß√£o cl√≠nica de um paciente pedi√°trico com terminologia m√©dica especializada.';
    let openaiModel = 'gpt-4o-transcribe';

    if (promptData && !promptError) {
      promptContent = promptData.prompt_content || promptContent;
      openaiModel = promptData.openai_model || openaiModel;
    }

    // AI dev note: Determinar extens√£o correta baseada no tipo MIME
    // OpenAI Whisper funciona melhor com formatos padr√£o
    let fileExtension = 'webm';
    let fileName = 'audio.webm';

    if (audioType.includes('webm')) {
      fileExtension = 'webm';
      fileName = 'audio.webm';
    } else if (audioType.includes('mp4') || audioType.includes('m4a')) {
      fileExtension = 'mp4';
      fileName = 'audio.mp4';
    } else if (audioType.includes('mp3') || audioType.includes('mpeg')) {
      fileExtension = 'mp3';
      fileName = 'audio.mp3';
    } else if (audioType.includes('wav')) {
      fileExtension = 'wav';
      fileName = 'audio.wav';
    } else if (audioType.includes('ogg')) {
      fileExtension = 'ogg';
      fileName = 'audio.ogg';
    }

    console.log(
      `üìù Arquivo para Whisper: ${fileName}, Extens√£o: ${fileExtension}`
    );
    console.log(
      `üìù Prompt configurado: ${promptContent ? `"${promptContent.substring(0, 100)}..."` : 'Nenhum'}`
    );

    // Prepare OpenAI FormData
    const openaiFormData = new FormData();
    const audioFile = new File([audioBlob], fileName, {
      type: audioType,
    });
    openaiFormData.append('file', audioFile);
    openaiFormData.append('model', openaiModel);
    openaiFormData.append('response_format', 'json');
    openaiFormData.append('language', 'pt');

    // AI dev note: Comportamento do prompt varia por modelo:
    // - whisper-1: Aceita apenas exemplos de texto/vocabul√°rio (224 tokens max)
    // - gpt-4o-mini-transcribe/gpt-4o-transcribe: Aceita contexto descritivo
    if (promptContent && promptContent.trim() !== '') {
      console.log(`‚úÖ Adicionando prompt ao modelo ${openaiModel}`);
      console.log(
        `üìù Tipo de prompt: ${openaiModel.includes('whisper') ? 'exemplo de texto' : 'contexto descritivo'}`
      );
      openaiFormData.append('prompt', promptContent);
    }

    // Call OpenAI API
    console.log('üöÄ Iniciando chamada para OpenAI Whisper...');
    const startTime = Date.now();

    const openaiResponse = await fetch(
      'https://api.openai.com/v1/audio/transcriptions',
      {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${openaiKey}`,
        },
        body: openaiFormData,
        signal: AbortSignal.timeout(60000),
      }
    );

    const duration = Date.now() - startTime;
    console.log(`‚è±Ô∏è Tempo de resposta do Whisper: ${duration}ms`);

    if (!openaiResponse.ok) {
      const errorText = await openaiResponse.text();
      console.error(`‚ùå OpenAI API Error: Status ${openaiResponse.status}`);
      console.error(`‚ùå Detalhes do erro: ${errorText.substring(0, 500)}`);
      throw new Error(
        `OpenAI API Error: ${openaiResponse.status} - ${errorText.substring(0, 200)}`
      );
    }

    const openaiData = await openaiResponse.json();
    const transcription = openaiData.text?.trim();

    console.log(
      `üìä Resposta do Whisper recebida. Tamanho da transcri√ß√£o: ${transcription?.length || 0} caracteres`
    );
    console.log(
      `üìù Primeiros 100 caracteres: ${transcription?.substring(0, 100) || '(vazio)'}`
    );

    if (!transcription) {
      console.error('‚ùå Transcri√ß√£o vazia ou n√£o encontrada na resposta');
      console.error(
        'üìã Resposta completa do OpenAI:',
        JSON.stringify(openaiData, null, 2)
      );
      throw new Error('Transcri√ß√£o n√£o encontrada na resposta da OpenAI');
    }

    console.log('‚úÖ Transcri√ß√£o bem-sucedida!');

    return new Response(
      JSON.stringify({
        success: true,
        transcription: transcription,
        audioSize: audioBlob.size,
        metadata: {
          model: openaiModel,
          promptSource: 'supabase',
        },
      } as TranscribeResponse),
      {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      }
    );
  } catch (error) {
    console.error('‚ùå Transcribe Audio Error:', error);

    return new Response(
      JSON.stringify({
        success: false,
        error: error.message || 'Erro interno do servidor',
      } as TranscribeResponse),
      {
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      }
    );
  }
});
